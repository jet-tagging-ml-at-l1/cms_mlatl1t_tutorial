{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install some dependencies\n",
    "%pip install pyarrow hls4ml pyparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c5bc4",
   "metadata": {},
   "source": [
    "\n",
    "# Quantization aware training with QKeras\n",
    "\n",
    "Quantization is a powerful way to reduce model memory and resource consumption. In this tutorial, we will use the libary QKeras to perform quantization aware training (QAT).\n",
    "\n",
    "In contrast to in Keras, where models are trained using floating point precision, QKeras quantizes each of the model weights and activation functions during training, allowing the network to adapt to the numerical precision that will eventually be used on hardware.\n",
    "\n",
    "During the forward pass of the network, each floating point weight is put into one of $2^{bitwidth}$ buckets. Which one it goes into is defined through rounding and clipping schemes.\n",
    "\n",
    "Below you can see an example of a tensor with a (symmetric) dynamic range of $x_{f}$ $[-amax, amax]$ mapped through quantization to a an 8 bit integer, $2^8=256$ discrete values in the interval $[-128, 127]$ (32-bit floating-point can represent ~4B numbers in the interval $[-3.4e38, 3.40e38]$).\n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/8-bit-signed-integer-quantization.png?ref_type=heads\" width=\"800\"/>\n",
    "\n",
    "Quantization of floating point numbers can be achieved using the quantization operation\n",
    "\n",
    "$$x_{q} = Clip(Round(x_{f}/scale))$$\n",
    "\n",
    "where $x_{q}$ is the quantized digit and $x_{f}$ is the floating point digit. $Round$ is a function that applies some rounding scheme to each number and $Clip$ is a function that clips outliers that fall outside the $[-128, 127]$ interval. The $scale$ parameter is obtained by dividing the float-point dynamic-range into 256 equal parts.\n",
    "\n",
    "On FPGA, we do not use int8 quantization, but fixed-point quantization, bu the idea is similar. Fixed-point representation is a way to express fractions with integers and offers more control over precision and range. We can split the $W$-bits making up an integer (in our case $W=8$) to represent the integer part of a number and the fractional part of the number. We usually reserve 1-bit representing the sign of the digit. The radix splits the remaining $W-1$ bits to $I$ most significant bits representing the integer value and $F$ least significant bits representing the fraction. We write this as $<W,I>$, where $F=W-1-I$.  Here is an example for an unsigned $<8,3>$:\n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/fixedpoint.png?ref_type=heads\" width=\"400\"/>\n",
    "\n",
    "\n",
    "This fixed point number corresponds to $2^4\\cdot0+2^3\\cdot0+2^2\\cdot0+2^1\\cdot1+2^0\\cdot0+2^{-1}\\cdot1+2^{-2}\\cdot1+2^{-3}\\cdot0=2.75$.\n",
    "\n",
    "The choice of $I$ and $F$ has to be derived as a trade-off between representation range and precision, where $I$ controls the range and $F$ the precision.\n",
    "\n",
    "In the following we will use a bitwidth of 8 and 0 integer bits. Not considering the sign bit, this means that the smallest number you can represent (the precision) and the largest number (the range) is:\n",
    "\n",
    "$$ \\rm{Precision}= \\frac{1}{2^{F}}= \\frac{1}{2^8} = 0.00390625$$\n",
    "$$\\rm{Range}= [-2^0,-2^0-1]=[-1,0] $$\n",
    "With zero integer bits the largest number you can represent is just below (but not including) 1. For weights in deep neural networks, being constrained to be less than 1 is often a reasonable assumtion.\n",
    "\n",
    "\n",
    "\n",
    "What QKeras (and other QAT libraries) do, is to include the quantization error during the training, in the following way:\n",
    "- \"Fake quantize\" the floating-point weights and activations during the forward pass: quantize the weights and use them for the layer operations\n",
    "- Immediately un-quantize the parameters so the rest of the computations take place in floating-point\n",
    "- During the backward pass, the gradient of the weights is used to update the floating point weight\n",
    "- The quantization operation gradient (zero or undefined) is handled by passing the gradient through as is (\"straight through estimator\")\n",
    "\n",
    "## Inspect the original model\n",
    "In the following we will use the QKeras library to add quantizers to our model. First, let's load the baseline model and remind ourselves what the architecture looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = '/eos/home-t/thaarres/cms_mlatl1t_tutorial/full_model.h5'\n",
    "baseline_model = load_model(model_path)\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84f91a",
   "metadata": {},
   "source": [
    "So we have 3 hidden layers with [64,32,32] neurons. We don't see it here, but they are all followed by an \"elu\" activation. The output is one node activated by a sigmoid activation function.\n",
    "\n",
    "## Translating to a QKeras QAT model\n",
    "There are two ways to translate this into a QKeras model that can be trained quantization aware, lets first do it manually:\n",
    "\n",
    "### Manual QKeras model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5073f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "\n",
    "input_size=26\n",
    "\n",
    "# Define the input layer\n",
    "inputs = Input(shape=(input_size,))\n",
    "\n",
    "# Define the three hidden layers and output layer\n",
    "hidden1 = QDense(\n",
    "        64,\n",
    "        name='qd1',\n",
    "        kernel_quantizer=quantized_bits(bits=8, integer=0, symmetric=0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(bits=8, integer=0, symmetric=0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        ) (inputs)\n",
    "hidden1 = QActivation(activation=quantized_relu(8), name='qrelu1')(hidden1)\n",
    "hidden2 = QDense(\n",
    "        32,\n",
    "        name='qd2',\n",
    "        kernel_quantizer=quantized_bits(bits=8, integer=0, symmetric=0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(bits=8, integer=0, symmetric=0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        ) (hidden1)\n",
    "hidden2 = QActivation(activation=quantized_relu(8), name='qrelu2')(hidden2)\n",
    "hidden3 = QDense(\n",
    "        32,\n",
    "        name='qd3',\n",
    "        kernel_quantizer=quantized_bits(bits=8, integer=0, symmetric=0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(bits=8, integer=0, symmetric=0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        ) (hidden2)\n",
    "hidden3 = QActivation(activation=quantized_relu(8), name='qrelu3')(hidden3)\n",
    "# Define the output layer with a single node, let's be careful with quantizing this one and be a bit more generous\n",
    "# Some prefer to leave this a Keras Dense layer, but then it requires more manual tuning in the hs4ml part\n",
    "logits = QDense(1, \n",
    "        name='logits',\n",
    "        kernel_quantizer=quantized_bits(bits=13, integer=0, symmetric=0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(bits=13, integer=0, symmetric=0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        ) (hidden3)\n",
    "\n",
    "output = Activation(activation='sigmoid', name='output')(logits)\n",
    "# Create the model\n",
    "qmodel = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Model summary\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c429c55",
   "metadata": {},
   "source": [
    "Wait! What is going on here?\n",
    "The magic happens in ```quantized_bits``` (see implementation [here](https://github.com/google/qkeras/blob/master/qkeras/quantizers.py#L1245)), where the parameters are the following:\n",
    "- ```bits```: The bitwidth, allowing you to have $2^{bits}$ unique values of each weight parameter\n",
    "- ```integers```: How many are integer bits, in this case zero. All 8 bits are used to represent the fractional part of the weight parameter, with no bits dedicated to representing whole numbers. This forces the value to be between -1 and 1. For DNNs this can be useful because the focus is entirely on the precision of the fraction rather than the magnitude of the number. Question: Would this also work on the output node if your algorithm is a regression of the jet mass?\n",
    "- ```symmetric```: should the values be symmetric around 0? In this case it doesnt have to be.\n",
    "- ```alpha```: with $2^W$ unique values available, we could let them go from [-2^W, 2^W-1] like above, but we can also let them go from $[-2^W*\\alpha, (2^W-1)*\\alpha]$. ```alpha``` is a scaling of the weights. Enabling this often leads to improved performance, but it doesnt talk so nicely to hls4ml, so we recommend leaving it at 1 (or get ready for having to debug)\n",
    "\n",
    "Having added this, QKeras will automatically apply fake quantization for us during the forward pass, accounting for the quantization error and returning a network that is optimized for the precision you plan on using in hardware.\n",
    "\n",
    "Another thing to notice is that we leave the sigmoid and the final output logit unquantized. This is because this is were we want the values to be very accurate, and it is not going to save us a lot of resources quantizing it.\n",
    "\n",
    "\n",
    "### Automatic model quantization through config\n",
    " You can also set the quantization for the full model using a model configuration. Sometimes this can be sater if you're using the same quantizer for all layers of the same type. You don't have to use this for this tutorial, we already have a model, but we will leave it here as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b138d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoQuant = False\n",
    "\n",
    "if autoQuant:\n",
    "    # Fine grained, per-layer control\n",
    "    #  config = {\n",
    "    #  \"example_model_topo_fc1\": {\n",
    "    #      \"kernel_quantizer\": \"quantized_bits(8,0,1)\",\n",
    "    #      \"bias_quantizer\": \"quantized_bits(8,0,1)\",\n",
    "    #   },                                                           \n",
    "    #  \"example_model_topo_activation1\": \"quantized_relu(8)\",                                         \n",
    "\n",
    "    #  \"example_model_topo_fc2\": {\n",
    "    #       \"kernel_quantizer\": \"quantized_bits(8,0,1)\",\n",
    "    #       \"bias_quantizer\": \"quantized_bits(8,0,1)\",\n",
    "    #   },                                                               \n",
    "    #  \"example_model_topo_activation2\": \"quantized_relu(8)\",                                                                                                     \n",
    "    #  \"example_model_topo_fc3\": {\n",
    "    #       \"kernel_quantizer\": \"quantized_bits(8,0,1)\",\n",
    "    #       \"bias_quantizer\": \"quantized_bits(8,0,1)\",\n",
    "    #   },                                                                                                     \n",
    "    #  example_model_topo_activation3: \"quantized_relu(8)\", \n",
    "    #  }      \n",
    "    # Coarse grained, per-layertype quantization\n",
    "    config = {\n",
    "      \"QDense\": {\n",
    "          \"kernel_quantizer\": \"quantized_bits(bits=8, integer=0, symmetric=0, alpha=1)\",\n",
    "          \"bias_quantizer\": \"quantized_bits(bits=8, integer=0, symmetric=0, alpha=1)\",\n",
    "      },\n",
    "      \"QActivation\": { \"relu\": \"quantized_relu(8)\" }\n",
    "    }\n",
    "    from qkeras.utils import model_quantize\n",
    "\n",
    "    qmodel = model_quantize(model, config, 4, transfer_weights=True)\n",
    "\n",
    "    for layer in qmodel.layers:\n",
    "        if hasattr(layer, \"kernel_quantizer\"):\n",
    "            print(layer.name, \"kernel:\", str(layer.kernel_quantizer_internal), \"bias:\", str(layer.bias_quantizer_internal))\n",
    "        elif hasattr(layer, \"quantizer\"):\n",
    "            print(layer.name, \"quantizer:\", str(layer.quantizer))\n",
    "\n",
    "    print()\n",
    "    qmodel.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947eda4",
   "metadata": {},
   "source": [
    "But be careful that activation functions like softmax/sigmoid and perhaps logit layers you want to keep at full presision doesn't get quantized!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59eea22",
   "metadata": {},
   "source": [
    "## But how many bits?\n",
    "\n",
    "So now we know how to quantize our models, but how do we know wich precision to choose?\n",
    "Finding the best number of bits and integer bits to use is non-trivial, and there are two ways we recommend:\n",
    "- The easiest strategy is to scan over the possible bit widths from binary up to some maximum value and choose the smallest one that still has acceptable accuracy, and this is what we often do. \n",
    "Code for how to do this can be found [here](https://github.com/thesps/keras-training/blob/qkeras/train/train_scan_models.py#L16), and is illustrated below.\n",
    "For binary and ternary quantization, we use the special ```binary(alpha=1.0)(x)``` and ```ternary(alpha=1.0)(x)``` quantizers. \n",
    "\n",
    "<img src=\"images/quant_scan.png\" width=\"400\"/>\n",
    "\n",
    "- Another thing you can do is to use our library for automatic quantization, [AutoQKeras](https://github.com/google/qkeras/blob/master/notebook/AutoQKeras.ipynb), to find the optimal quantization for each layer. This runs hyperparameter optimisation over quantizers/nodes/filters simultenously. An example can be found at the end of [this notebook](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part6_cnns.ipynb) \"Bonus exercise: Automatic quantization with AutoQKeras\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da0954",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "Besides reducing the numerical precision of all the weights, biases and activations, I also want to remove neurons and synapses that do not contribute much to the network overall decision. We do that by pruning, let's remove 50\\% of the weights (spasity=0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fe6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "# The training step is one gradient update, or epochs*N_samples/batchsize\n",
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.5, begin_step=6000, frequency=10)}\n",
    "qmodel = prune.prune_low_magnitude(qmodel, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1494e",
   "metadata": {},
   "source": [
    "## Defining the data input type\n",
    "Great, we now have our model ready to be trained! There is one last important thing we have to think about and that is the *precision of the input*.  In the L1T, all of the inputs are quantized. For instance, the precision used for the GT is listed [here](https://github.com/cms-l1-globaltrigger/mp7_ugt_legacy/blob/master/doc/scales_inputs_2_ugt/pdf/scales_inputs_2_ugt.pdf).\n",
    "\n",
    "Ideally, when you train your network, you use the hardware values that the algorithm will actually receive when running inference in the trigger.\n",
    "\n",
    "We saw, however, that the inputs were all scaled to have a mean of zero and variance of one in the previous exercise. That means that the new optimal precision for the inputs have changes and you need to define what the precision will be. Here we will do it by inspection and intuition, and use the same precision for all of the input features. Let's now load, scale the data and look at the input value distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0355af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import pickle\n",
    "\n",
    "path = '/eos/home-t/thaarres/cms_mlatl1t_tutorial/'\n",
    "\n",
    "X_train = ak.from_parquet(path + \"/X_train.parquet\").to_numpy() \n",
    "X_test  = ak.from_parquet(path + \"/X_test.parquet\").to_numpy() \n",
    "\n",
    "y_train = ak.from_parquet(path + \"/y_train.parquet\").to_numpy()\n",
    "y_test  = ak.from_parquet(path + \"/y_test.parquet\").to_numpy()\n",
    "\n",
    "# In this case the test and train data is already scaled, but this is how you would laod and apply it:\n",
    "#Load the scaler and parameters and apply to the data\n",
    "scale = False\n",
    "if scale:\n",
    "    file_path = path+'/scaler.pkl'\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test  = scaler.transform(X_test);\n",
    "\n",
    "\n",
    "print(f\"Training on {X_train.shape[0]} events, represented by {X_train.shape[1]} input features\")\n",
    "print(f\"Testing on {X_test.shape[0]} events, represented by {X_test.shape[1]} input features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d20cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = 1024\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(X_train[:1000, :], bins=bins, stacked=True, label=[f'Input {i+1}' for i in range(X_train.shape[1])])\n",
    "\n",
    "plt.xlabel('Feature Value (standardized)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Stacked Histogram of all features')\n",
    "plt.legend(loc='upper right', ncol=2)\n",
    "plt.semilogy()\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275844b",
   "metadata": {},
   "source": [
    "In this case, the values seem to be <50 so lets assume 6 integer bits ($2^6=64$). The number of fractional bits will define our precision, and will affect the network performance. Let's assume 10 is sufficient (the smallest increment we can represent is $2^{-10}=0.0009765625$). To make our network adapt to this input precision, we need to \"treat\" our training and testing set with a quantizer to go from FP32 $\\rightarrow <16,6>$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7be5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_quantizer = quantized_bits(bits=16, integer=6, symmetric=0, alpha=1)\n",
    "qX_train = input_quantizer(X_train.astype(np.float32)).numpy()\n",
    "qX_test = input_quantizer(X_test.astype(np.float32)).numpy()\n",
    "\n",
    "# Save the quantized test data and labels to a numpy file, such that it can be used to test the firmware\n",
    "np.save('qX_test.npy', qX_test)\n",
    "np.save('qy_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de19ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(qX_train[:1000, :], bins=bins, stacked=True, label=[f'Input {i+1}' for i in range(X_train.shape[1])])\n",
    "\n",
    "plt.xlabel('Quantized Feature Value (standardized)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Stacked Histogram of all features')\n",
    "plt.legend(loc='upper right', ncol=2)\n",
    "plt.semilogy()\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e7438",
   "metadata": {},
   "source": [
    "The weight distribution looks similar, but we can not really say how much we loose in performance before training with different input precisions.\n",
    "\n",
    "Phew, okay, finally time to train. For this part there are 2 things to note: you need to add a pruning callback and also you might need to adjust the learning rate (like add a learning rate decay). Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('model_best_checkpoint.h5', save_best_only=True, monitor='val_loss')\n",
    "# This might result in returning a not fully pruned model, but that's okay!\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callbacks=[early_stopping, reduce_lr, model_checkpoint, pruning_callbacks.UpdatePruningStep()]\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "qmodel.compile(optimizer=adam, loss=['binary_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "qmodel.fit(qX_train, y_train, batch_size=2048, epochs=50,validation_split=0.20, shuffle=True,callbacks=callbacks,verbose=1) \n",
    "qmodel = strip_pruning(qmodel)\n",
    "qmodel.save('qtopo_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75409ec1",
   "metadata": {},
   "source": [
    "Before checking and comparing the accuracy, lets look at the weights and see if they look quantized and pruned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors  = ['#7b3294','#c2a5cf','#a6dba0','#008837']\n",
    "# TAKE EVERY OPPORTUNITY TO ADVERTISE COLORBLIND SAFE PLOTS :)\n",
    "\n",
    "allWeightsByLayer = {}\n",
    "for layer in qmodel.layers:\n",
    "    layername = layer._name\n",
    "    if len(layer.get_weights())<1:\n",
    "      continue\n",
    "    weights=layer.weights[0].numpy().flatten()  \n",
    "    allWeightsByLayer[layername] = weights\n",
    "    print('Layer {}: % of zeros = {}'.format(layername,np.sum(weights==0)/np.size(weights)))\n",
    "labelsW = []\n",
    "histosW = []\n",
    "  \n",
    "for key in reversed(sorted(allWeightsByLayer.keys())):\n",
    "    labelsW.append(key)\n",
    "    histosW.append(allWeightsByLayer[key])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.semilogy()\n",
    "plt.legend(loc='upper left',fontsize=15,frameon=False)\n",
    "bins = np.linspace(-1, 1, 1024) \n",
    "ax.hist(histosW,bins,histtype='stepfilled',stacked=True,label=labelsW,color=colors)#, edgecolor='black')\n",
    "ax.legend(frameon=False,loc='upper left')\n",
    "axis = plt.gca()\n",
    "plt.ylabel('Number of Weights')\n",
    "plt.xlabel('Weights')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd44876",
   "metadata": {},
   "source": [
    "This looks like expected! Now, lets compare the performance to that of the floating point model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = baseline_model.predict(X_test, batch_size = 4096)\n",
    "qy_pred = qmodel.predict(qX_test, batch_size = 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "assert(len(y_test) == len(y_pred) == len(qy_pred)), \"Inconsistent predicted and true!\"\n",
    "fpr, tpr, thr = roc_curve(y_test, y_pred, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "qfpr, qtpr, qthr = roc_curve(y_test, qy_pred, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
    "qroc_auc = roc_auc_score(y_test, qy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also convert from FPR to L1 rate:\n",
    "\n",
    "def totalMinBiasRate():\n",
    "\n",
    "    LHCfreq = 11245.6\n",
    "    nCollBunch = 2544\n",
    "\n",
    "    return LHCfreq * nCollBunch / 1e3 # in kHz\n",
    "fpr *= totalMinBiasRate()\n",
    "qfpr *= totalMinBiasRate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d20287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot it!\n",
    "f, ax  = plt.subplots(figsize=(8,6))\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14) \n",
    "ax.set_xlim(0,100)\n",
    "\n",
    "ax.plot(fpr, tpr, color='#7b3294', lw=2, ls='dashed', label=f'Baseline (AUC = {roc_auc:.5f})')\n",
    "ax.plot(qfpr, qtpr, color='#008837', lw=2, label=f'Quantized+Pruned (AUC = {qroc_auc:.5f})')\n",
    "ax.set_xlabel('L1 Rate (kHz)')\n",
    "ax.set_ylabel('Signal efficiency')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True)\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba99f2a",
   "metadata": {},
   "source": [
    "So it seems despite having reduced the numerical precision of the model and the input, as well as removing 50% of the model weights, we're doing pretty good! This can be tuned to get even better, by carefully adjusting the input precision and the model precision, especially increaseing the precision of the logit layer.\n",
    "\n",
    "# Generating firmware with\n",
    "\n",
    "<img src=\"images/hls4ml_logo.png\" width=\"400\"/>\n",
    "\n",
    "Time to translate this model into HLS (which we will integrate in the emulator) and use to generate the vhdl to be integrated in the trigger firmware.\n",
    "hls4ml seamlessly talks to QKeras, making our jobs way easier for us, but there is still some work for us to do to make sure we get good hardware model accuracy. Lets start!\n",
    "There are a few things I already know in advance and would like my model to include:\n",
    "- Be execuded fully parallel (=unrolled) to reach the lowest possible latency. We set the ReuseFactor=1 and Strategy=Latency\n",
    "- The correct input precision\n",
    "- The correct model output (that's something you have to figure out yourself!)\n",
    "- Use \"correct\" precisions to make sure the hardware model performs the same as the software one. QKeras handles weights/biases and activation functions for us, but there are a few other parameters that need to be set by hand\n",
    "\n",
    "For the final point, have a look at the following diagram:\n",
    "\n",
    "<img src=\"images/hls4ml_precisions.png\" width=\"400\"/>\n",
    "\n",
    "Whereas the $weight$ and $bias$ is set to its optimal value from the QKeras model, the accumulator $accum$ and $result$ is set to some default value that might not be optimal for a given model and might need tuning. Let's do a first attemt and compare the ROC curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "def print_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key), end='')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent + 1)\n",
    "        else:\n",
    "            print(':' + ' ' * (20 - len(key) - 2 * indent) + str(value))\n",
    "            \n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(qmodel, granularity='name')\n",
    "config[\"Model\"][\"Strategy\"] = \"Latency\"\n",
    "config[\"Model\"][\"ReuseFactor\"] = 1\n",
    "\n",
    "inputPrecision = \"ap_fixed<16,7,AP_RND,AP_SAT>\" #Adding one bit for the sign, different definitions QKeras/Vivado\n",
    "for layer in qmodel.layers:\n",
    "    if layer.__class__.__name__ in [\"InputLayer\"]:\n",
    "        config[\"LayerName\"][layer.name][\"Precision\"] = inputPrecision\n",
    "config[\"LayerName\"][\"output\"][\"Precision\"][\"result\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\"        \n",
    "\n",
    "# If the logit layer is a \"normal\" Keras kayer and has not been quantized during the training, \n",
    "# we need to be careful setting the accuracy. This can be done in the following way:\n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"weight\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"bias\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"accum\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"result\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(qmodel, \n",
    "                                                       hls_config=config, \n",
    "                                                       io_type='io_parallel', #other option is io_stream\n",
    "                                                       output_dir='L1TMLDemo_v1',\n",
    "                                                       project_name='L1TMLDemo_v1', \n",
    "                                                       part='xcu250-figd2104-2L-e', #Target FPGA, ideally you would use VU9P and VU13P that we use in L1T but they are not installed at lxplus, this one is close enought for this\n",
    "                                                       clock_period=2.5, # Target frequency 1/2.5ns = 400 MHz\n",
    "                                                       input_data_tb='qX_test.npy', # For co-simulation\n",
    "                                                       output_data_tb='qy_test.npy',# For co-simulation\n",
    ")\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00bf3f",
   "metadata": {},
   "source": [
    "First, what does our newly created model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc990262",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b95c4",
   "metadata": {},
   "source": [
    "Here you can see that the precision is what we set it to be in QKeras as well as what we set manually in the config. One thing to note is the different definitions used in QKeras and in ap_fixed:\n",
    "- ```quantized_bits(8,0) -> ap_fixed<8,1>```\n",
    "- ```quantized_relu(8,0) -> ap_ufixed<8,0>```\n",
    "Also you can see that the defualt value for result/accu is set to $16,6$. This can also be tuned to more optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also run predict on the C++ implementation of our model and make sure it's the ~same as for the QKeras model:\n",
    "# This is very slow for the C++ implementation of our model, so lets only do 1000 events for this\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(qX_test))\n",
    "\n",
    "print(f\"Truth labels:\\n {y_test[17:27]}\")\n",
    "print(f\"Qkeras prediction:\\n {qy_pred[17:27]}\")\n",
    "print(f\"HLS prediction:\\n {y_hls[17:27]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7480a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot it!\n",
    "hlsfpr, hlstpr, hlsthr = roc_curve(y_test, y_hls, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
    "hlsfpr *= totalMinBiasRate()\n",
    "hlsroc_auc = roc_auc_score(y_test, y_hls)\n",
    "\n",
    "f, ax  = plt.subplots(figsize=(8,6))\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14) \n",
    "ax.set_xlim(0,100)\n",
    "\n",
    "ax.plot(fpr, tpr, color='#7b3294', lw=2, ls='dashed', label=f'Baseline (AUC = {roc_auc:.5f})')\n",
    "ax.plot(qfpr, qtpr, color='#008837', lw=2, label=f'Quantized+Pruned (AUC = {qroc_auc:.5f})')\n",
    "ax.plot(hlsfpr, hlstpr, color='#a6dba0', lw=2, ls='dotted', label=f'HLS Quantized+Pruned (AUC = {hlsroc_auc:.5f})')\n",
    "ax.set_xlabel('L1 Rate (kHz)')\n",
    "ax.set_ylabel('Signal efficiency')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True)\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00985383",
   "metadata": {},
   "source": [
    "Oh! That was easier than expected. If you see the accuracies differing significantly, it's a good idea to look into accumulator and reult precisions. Also with tools like $Trace$ and $Profiling$ that you can learn from in the [official hls4ml tutorial](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part2_advanced_config.ipynb) can be helpful! In this case, it doesnt seem like it's necessary. Now let's build it! Lets run C-synthesis (C++ to register-transfer level), Vivado logic synthesis (gate level representation) and co-simulation (send test vectors, do an exhaustive functional test of the implemented logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running synthesis!\")\n",
    "report = hls_model.build(csim=False, synth=True, vsynth=True, cosim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893c6cd",
   "metadata": {},
   "source": [
    "Now, lets, look at the reports! The latency can be extracted from the C-synthesis report, and validated from the co-simulation report (where actual data is sent through the logic. The resource consumption can be extracted from the implementation report (Vivado logic synthesis) and is more accurate then what is quoted in the C-synthesis report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nC synthesis report (latency estimate):\")\n",
    "print_dict(report[\"CSynthesisReport\"])\n",
    "#print_dict(report[\"CosimReport\"]) Not working due to missing libc header sys/cdefs.h :(?\n",
    "print(\"\\nVivado synthesis report (resource estimates):\")\n",
    "print_dict(report[\"VivadoSynthReport\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1723a",
   "metadata": {},
   "source": [
    "A latency of $2.5\\cdot16=40$ ns, that is not bad! Also, the network is using very little resources: 8k out of 1728k LUTs, 15 out of 12k DSPs. This is <1% of the total available resources.  We have a set of HLS files that will be integrated into the CMSSW emulator (```L1TMLDemo_v1/firmware/```) and VHDL that will be integrated into the mGT firmware (```L1TMLDemo_v1/myproject_prj/solution1/impl/vhdl/```). That's next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlatl1",
   "language": "python",
   "name": "mlatl1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
