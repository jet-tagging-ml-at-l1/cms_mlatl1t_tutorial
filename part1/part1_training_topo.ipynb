{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f1a6d0",
   "metadata": {},
   "source": [
    "**Execute the below cell directly â€“ might take ~1min to import everything!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle, uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import mplhep as hep\n",
    "mpl.rcParams.update({'font.size': 20})\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd06c11",
   "metadata": {},
   "source": [
    "# Intro: NN topological trigger for L1\n",
    "\n",
    "See the recent Deep Dive for an overview of the concepts and current studies of ML triggers at L1:\n",
    "* AXOL1TL: anomaly trigger in the uGT: https://indico.cern.ch/event/1342837/#34-axol1tl-event-level-anomaly\n",
    "* CICADA: anomaly trigger using L1 Calorimeter tower images: https://indico.cern.ch/event/1342837/#35-cicada\n",
    "* TOPO: ML topo classifier in the uGT: https://indico.cern.ch/event/1342837/#33-topo-ml-based-topology-trig\n",
    "\n",
    "The goal of this part of the ML@L1 tutorial is to get you familiarised with the Run3 L1 Global Trigger (uGT) data and learn how to use it to train a ML trigger for signal vs background classification -> the TOPO approach from above.\n",
    "\n",
    "This part1 of the tutorial consists of the following steps:\n",
    "\n",
    "1. Load L1 data from NanoAOD files and preprocess them for ML usage\n",
    "2. Train and evaluate an NN classifier\n",
    "3. Check the signal efficiency & rate of \"traditional\" L1 seeds (the menu)\n",
    "4. Compare the NN trigger to the L1 menu\n",
    "5. Measure efficiencies wrt some offline object variables\n",
    "\n",
    "For an intro to the CMS NanoAOD dataformat and it's usage for trigger-related measurements, see the recent PO&DAS tutorial here: https://gitlab.cern.ch/cms-podas23/dpg/trigger-exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af57345",
   "metadata": {},
   "source": [
    "# 1. Load L1 objects data and plot variables for sig vs bkg\n",
    "\n",
    "We will be using samples from the same Run3Summer22 MC campaign for this tutorial:\n",
    "* HH>2b2tau as signal (`/GluGlutoHHto2B2Tau_kl-1p00_..._TuneCP5_13p6TeV_powheg-pythia8/Run3Summer22...`)\n",
    "* SingleNu gun as background (`/SingleNeutrino_E-10_gun/Run3Summer22MiniAODv4-SNB_130X_mcRun3_2022_..`)\n",
    "\n",
    "Note that the single neutrino gun is the MC equivalent of the `ZeroBias` data, but we will be using only MC for easier comparisons (e.g. the same PU profile and L1 calibrations).\n",
    "\n",
    "Below are the data paths to the NanoAOD files with the L1 objects added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_sig = \"/eos/cms/store/group/dpg_trigger/comm_trigger/L1Trigger/alobanov/run3/L1nano/ggHH2b2tau_130x_v12.root\"\n",
    "fname_bkg = \"/eos/cms/store/group/dpg_trigger/comm_trigger/L1Trigger/alobanov/run3/L1nano/v12/mc/SingleNeutrino_E-10_gun/SingleNeutrino_130X_L1NanoV12_None/231205_110830/SingleNeutrino_130X_L1NanoV12.root\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e035d62",
   "metadata": {},
   "source": [
    "We will be using [`uproot`](https://uproot.readthedocs.io/en/latest/index.html) and [`awkward arrays`](https://awkward-array.org/doc/main/) for reading the Nano ROOT files and manipulating the \"jagged\" arrays.\n",
    "\n",
    "(see the above linked PO/DAS tutorial for more intro material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8071cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sig = uproot.open(fname_sig)\n",
    "f_bkg = uproot.open(fname_bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f735cc1",
   "metadata": {},
   "source": [
    "We will be reading the L1 collections of:\n",
    "* `L1Mu`: muons\n",
    "* `L1EG`: e/gamma\n",
    "* `L1Jet`: jets\n",
    "* `L1EtSum`: sums such as MET, HT\n",
    "\n",
    "For all we want to get the `pt`, `phi` and `eta` (except for sums), and in addition the sum type (usage will be explained below).\n",
    "\n",
    "See the [nano auto-doc](https://cms-nanoaod-integration.web.cern.ch/autoDoc/NanoAODv12/2023Prompt/doc_EGamma0_Run2023C-PromptNanoAODv12_v2-v2.html) for more information.\n",
    "\n",
    "Below we will load the information using the `array` method of `awkward`. It accepts regexp for the branch name filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_sig = f_sig[\"Events\"].arrays(\n",
    "    filter_name = \"/(L1Mu|L1EG|L1Jet|L1EtSum)_(pt|eta|phi|etSumType)/\", \n",
    "#     entry_stop = 1e1, \n",
    "    how = \"zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd923e1",
   "metadata": {},
   "source": [
    "Note how each object is grouped in a respective field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sig.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450afb99",
   "metadata": {},
   "source": [
    "And the variables are accesible at the next level (this is a result of using `how=\"zip\"` when loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sig.L1EG.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34753916",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_bkg = f_bkg[\"Events\"].arrays(\n",
    "    filter_name = \"/(L1Mu|L1EG|L1Jet|L1EtSum)_(pt|eta|phi|etSumType)/\", \n",
    "#     entry_stop = 1e1, \n",
    "    how = \"zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bkg.L1EG.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629046d",
   "metadata": {},
   "source": [
    "### Important: in DATA one has to only filter on `bx=0` for proper usage!\n",
    "Below is an example of how this can be achieved:\n",
    "\n",
    "We shall filter the `bx=0` L1 objects from the events -> need to add `bx` to the array reading step above.\n",
    "\n",
    "```python\n",
    "data_bkg_allBX = data_bkg\n",
    "\n",
    "bx = 0\n",
    "data_bkg_bx0 = {}\n",
    "\n",
    "for obj in data_bkg_allBX.fields:\n",
    "    # make a mask to filter out bx=0 only\n",
    "    bx_mask = data_bkg_allBX[obj].bx == bx\n",
    "    data_bkg_bx0[obj] = data_bkg_allBX[obj][bx_mask]\n",
    "    \n",
    "data_bkg = ak.Array(data_bkg_bx0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e44ca",
   "metadata": {},
   "source": [
    "## Inspect input data / plot distributions\n",
    "\n",
    "First, let's inspect the variables we are going to use. \n",
    "\n",
    "We will start with the object multiplicities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a5055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for obj in [ 'L1EG', 'L1Jet', 'L1Mu']:\n",
    "    plt.figure()#figsize = (8,4))\n",
    "\n",
    "    for label, data in zip([\"bkg\",\"sig\"], [data_bkg, data_sig]):\n",
    "        num = ak.num(data[obj])\n",
    "        plt.hist(num, label = label, bins = range(13), density = True, \n",
    "                 #log = True, \n",
    "                 histtype = \"step\")\n",
    "        \n",
    "    plt.xlabel(f\"N of {obj}\")\n",
    "    plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6fd72",
   "metadata": {},
   "source": [
    "Let us also plot the pt of the leading object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669e0b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj in [ 'L1EG', 'L1Jet', 'L1Mu']:\n",
    "    plt.figure()#figsize = (8,4))\n",
    "\n",
    "    for label, data in zip([\"bkg\",\"sig\"], [data_bkg, data_sig]):\n",
    "        # notice the [:,:1] below -> we slice the array and select no more than the first entry per event\n",
    "        # ak.ravel makes the array flat such that we can fill a histogram\n",
    "        plt.hist(ak.ravel(data[obj].pt[:,:1]), label = label, bins = 50, density = True, \n",
    "                 log = True, \n",
    "                 histtype = \"step\")\n",
    "        \n",
    "    plt.xlabel(f\"{obj} pt\")\n",
    "    plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9c4ad",
   "metadata": {},
   "source": [
    "There surely is some discrimation power in these variables! :)\n",
    "\n",
    "Let's move over to the ML part, though we first need to preprocess the data / convert it into a flat format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8c026",
   "metadata": {},
   "source": [
    "### Convert data to regular array for ML usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b235c3",
   "metadata": {},
   "source": [
    "Due to the variable number of objects per event (except for the sums) we need to convert the data from the jagged format to a tabular (regular) format with the rows corresponding to events and the columns to the variables. \n",
    "\n",
    "For that we have to pick:\n",
    "1. An order of objects e.g. Sums, Jets, EG, Muons\n",
    "2. The variables for each\n",
    "3. The max number of objects we want to use\n",
    "4. The values to use to pad empty objects\n",
    "\n",
    "We can write this down in the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [\n",
    "    {\"name\" : \"MET\", \"key\" : \"L1EtSum\", \"fields\" : [\"pt\", \"phi\"], \"n_obj\" : 1,\n",
    "     \"cuts\" : [{\"cuttype\" : \"equals\", \"field\" : \"etSumType\", \"value\" : 2}] },  # etSumType 2 = MET noHF\n",
    "    {\"name\" : \"jet\", \"key\" : \"L1Jet\", \"fields\" : [\"pt\", \"eta\", \"phi\"], \"n_obj\" : 8},\n",
    "    {\"name\" : \"eg\", \"key\" : \"L1EG\", \"fields\" : [\"pt\", \"eta\", \"phi\"], \"n_obj\" : 8},\n",
    "    {\"name\" : \"muon\", \"key\" : \"L1Mu\", \"fields\" : [\"pt\", \"eta\", \"phi\"], \"n_obj\" : 2},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e37ba",
   "metadata": {},
   "source": [
    "Note that for getting L1MET from the sum collection we need to filter based on the `sumType`. \n",
    "`etSumType==2` corresponds to MET (w/o HF) as can be checked in the corresponding CMSSW dataformat:\n",
    "https://github.com/cms-sw/cmssw/blob/master/DataFormats/L1Trigger/interface/EtSum.h#L36\n",
    "\n",
    "_(note though that normally METHF/ETMHF is used for MET triggers -> `type==8`)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff17012",
   "metadata": {},
   "source": [
    "We will use dedicated functions to ease this conversion from jagged to regular format.\n",
    "\n",
    "These functions are taken from the end-to-end ML@L1 framework of the TOPO team:\n",
    "(authors: `@flabe` and `@alobanov`)\n",
    "\n",
    "https://gitlab.cern.ch/uhh-l1t/uhh_mlatl1_run3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPadNParr(events, obj, n_pad, fields, cuts = None, name = None, pad_val = 0):\n",
    "    '''\n",
    "    This function filter objects and pads them to a certain length with a given value\n",
    "    '''\n",
    "    \n",
    "    objects = events[obj]\n",
    "    \n",
    "    if not name: name = obj\n",
    "    \n",
    "    # cuts are defined as a dictionary containing the relevant keys:\n",
    "    # cuttype, field and value\n",
    "    if cuts:\n",
    "        for cut in cuts:\n",
    "            if cut[\"cuttype\"] == \"equals\": objects = objects[objects[cut[\"field\"]] == cut[\"value\"]]\n",
    "            else: raise Exception(\"Cuttype {} is not implemented.\".format(cut[\"cuttype\"]))\n",
    "    \n",
    "    pad_arrs = []\n",
    "    var_names = []\n",
    "        \n",
    "    # padding with nones\n",
    "    pad_arr = ak.pad_none(objects, n_pad, clip=True)\n",
    "    \n",
    "    # combining to numpy\n",
    "    for i in range(n_pad):\n",
    "\n",
    "        for var in fields:\n",
    "            pad_arrs += [ak.to_numpy( ak.fill_none(pad_arr[var][:,i], pad_val) )]\n",
    "            var_names.append( \"{}_{}_{}\".format(name, i, var) )\n",
    "            \n",
    "    return np.stack(pad_arrs), var_names\n",
    "\n",
    "def formatDataTopotrigger(data, objects, verbosity = 0):\n",
    "    '''\n",
    "    This function concatenates the padded arrays for different objects.\n",
    "    It is controlled via a dictionary as defined above\n",
    "    '''\n",
    "    \n",
    "    # this will be filled by all required objects\n",
    "    dataList = [] \n",
    "    varList = []\n",
    "    \n",
    "    for obj in objects: \n",
    "        dat, names = getPadNParr(data, obj[\"key\"], obj[\"n_obj\"], obj[\"fields\"], obj[\"cuts\"] if \"cuts\" in obj else None, obj[\"name\"] )\n",
    "        dataList.append(dat)\n",
    "        varList += names\n",
    "        \n",
    "    if verbosity > 0:\n",
    "        print(\"The input variables are the following:\")\n",
    "        print(varList)\n",
    "                \n",
    "    # combining and returning (and transforming back so events are along the first axis...)\n",
    "    return np.concatenate(dataList, axis = 0).T, varList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03383b",
   "metadata": {},
   "source": [
    "Let's first convert the signal data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig, var_names = formatDataTopotrigger(data_sig, objects, verbosity = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbc7e8",
   "metadata": {},
   "source": [
    "You can see that we have ~100k events and 50 variables.\n",
    "Let us do the same for the background sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bkg, var_names = formatDataTopotrigger(data_bkg, objects, verbosity = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f68b8",
   "metadata": {},
   "source": [
    "And now we can compare the distributions of the input features. \n",
    "We will focus on the leading objects only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d6c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate(var_names[:100]):\n",
    "    if \"_0_\" not in name: continue\n",
    "        \n",
    "    plt.figure()\n",
    "    \n",
    "    _ = plt.hist(x_bkg[:,i], bins = 50, log = True, density = True, label = \"Bkg\")\n",
    "    _ = plt.hist(x_sig[:,i], bins = _[1], histtype = \"step\", density = True, label = \"Sig\")\n",
    "    \n",
    "    plt.xlabel(name)\n",
    "    plt.legend()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d20d9",
   "metadata": {},
   "source": [
    "You can see now that for the non-sum objects we have peaks at 0 as this was the `pad_val` padding value for filling empty variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d5973",
   "metadata": {},
   "source": [
    "### Final preprocessing: concatenating sig/bkg, scaling and train/test splitting\n",
    "\n",
    "Finally, we have to concatenate the signal and background arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels\n",
    "y_bkg = np.zeros(len(x_bkg))\n",
    "y_sig = np.ones(len(x_sig))\n",
    "\n",
    "# combining signal & bkg\n",
    "x = np.concatenate((x_bkg, x_sig))\n",
    "y = np.concatenate((y_bkg, y_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca98ad2",
   "metadata": {},
   "source": [
    "Then, we need to scale / normalise the inputs to be able to use them together in the NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d879d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "_ = scaler.fit(x)\n",
    "x_scaled = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebeb6e2",
   "metadata": {},
   "source": [
    "We will save the scaler and data for the next parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory\n",
    "outdir = \"part1_outputs/\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+\"scaler.pkl\", 'wb') as file_pi: \n",
    "    pickle.dump(scaler, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba9a91",
   "metadata": {},
   "source": [
    "And split dataset to train/test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b321d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539cd0e",
   "metadata": {},
   "source": [
    "We will save the train/test data now using awkward's `to_parquet` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7258b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.to_parquet(X_train, outdir+\"X_train_scaled.parquet\")\n",
    "ak.to_parquet(y_train, outdir+\"y_train_scaled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.to_parquet(X_test, outdir+\"X_test_scaled.parquet\")\n",
    "ak.to_parquet(y_test, outdir+\"y_test_scaled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lthr part1_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b90b6",
   "metadata": {},
   "source": [
    "These files can be read with `ak.read_parquet(\"filename.parquet\")` afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdee613",
   "metadata": {},
   "source": [
    "# 2. Train NN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb82f9",
   "metadata": {},
   "source": [
    "Now we can move one to the NN training. \n",
    "We will use a simple fully-connected 3 layer network with 64/32/32 nodes following the HLS4ML tutorial: https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part1_getting_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac16b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu1'))\n",
    "model.add(Dense(32, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu2'))\n",
    "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu3'))\n",
    "model.add(Dense(1, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='sigmoid', name='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=adam, loss=['binary_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269f965",
   "metadata": {},
   "source": [
    "Next, we will train the model with 10 epochs and record the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276a245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=1024,\n",
    "    epochs=10,\n",
    "    validation_split=0.25,\n",
    "    shuffle=True,\n",
    "#     callbacks=callbacks.callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bf943",
   "metadata": {},
   "source": [
    "We will save the model and history too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(outdir + \"/model.h5\")\n",
    "\n",
    "with open(outdir + \"/history.pkl\", 'wb') as file_pi: pickle.dump(history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c5389",
   "metadata": {},
   "source": [
    "Below is a convenience function to plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingHistory(history, metrics = [\"loss\", \"accuracy\"], f = None, axs = None):\n",
    "    \n",
    "    # creating the plot\n",
    "    if not f and not axs:\n",
    "        f, axs = plt.subplots(len(metrics), 1, figsize = (12, 4*len(metrics)), sharex = True)\n",
    "    if len(metrics) == 1:\n",
    "        axs = [axs]\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    # labeling\n",
    "#     hep.cms.label(\"private work\", data=False, ax=axs[0])\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "        \n",
    "        metric = metrics[i]\n",
    "        ax = axs[i]\n",
    "        ax.set_ylabel(metric)\n",
    "        \n",
    "        if isinstance(history, list): # handle kfold\n",
    "            for foldi in range(len(history)):\n",
    "                ax.plot(history[foldi].history[metric], color = \"C{}\".format(foldi))\n",
    "                ax.plot(history[foldi].history['val_' + metric], color = \"C{}\".format(foldi), linestyle = \"--\")\n",
    "                \n",
    "            la2, = ax.plot([0,0], [0,0], color=\"Grey\")\n",
    "            lb2, = ax.plot([0,0], [0,0], color=\"Grey\", linestyle = \"--\")\n",
    "            ax.legend([la2, lb2], [\"training\", \"validation\"])\n",
    "        else: \n",
    "            xs = np.arange(len(history.history['val_' + metric]))\n",
    "            ax.plot(xs,history.history[metric], label = 'training')\n",
    "            ax.plot(xs+.5, history.history['val_' + metric], label= 'validation')\n",
    "            ax.legend()\n",
    "\n",
    "    axs[-1].set_xlabel(\"Epoch\")\n",
    "    \n",
    "    return f, axs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b347b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotTrainingHistory(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd1687",
   "metadata": {},
   "source": [
    "We can see that the model converges nicely and does not seem to overtrain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b83c3",
   "metadata": {},
   "source": [
    "### Make ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a67602",
   "metadata": {},
   "source": [
    "Next, we will make a ROC curve for the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bdf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions:\n",
    "ak.to_parquet(y_test_pred, outdir+\"y_test_pred.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea324fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_test_pred, drop_intermediate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef48de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR: background efficiency\")\n",
    "plt.ylabel(\"TPR: signal efficiency\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c975c",
   "metadata": {},
   "source": [
    "The AUC seems incredibly high! \n",
    "\n",
    "Though remember that at the L1 trigger level we look for reduction factors 1e4 (40 MHz -> 1 kHz), i.e. the FPR of interest lies much lower.\n",
    "\n",
    "Much more practical is the rate in [k]Hz. In order to compute it we need to multiply the FPR with the LHC revolution frequency ~11kHz and the number of colliding bunches. The later can be looked up in the [Fill Report on OMS](https://cmsoms.cern.ch/cms/fills/report?cms_fill=9043) and is normally about 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb46923",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for L1 rate estimates from ZeroBias/SingleNuMC\n",
    "def totalMinBiasRate(nCollBunch = 2500):\n",
    "    LHCfreq = 11245.6\n",
    "    return LHCfreq * nCollBunch / 1e3 # in kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfa75a",
   "metadata": {},
   "source": [
    "Let's now make the ROC plot and focus on a rate range 0-100 kHz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600afc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr * totalMinBiasRate(), tpr)\n",
    "plt.xlabel(\"FPR*MBrate: Trigger rate [kHz]\")\n",
    "plt.ylabel(\"TPR: signal efficiency\")\n",
    "plt.grid()\n",
    "plt.xlim(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33242863",
   "metadata": {},
   "source": [
    "Voila! With 100 kHz our NN trigger has almost 100% signal efficiency :)\n",
    "But even at rather low rates the signal acceptance is quite significant.\n",
    "\n",
    "### In order to get a better judgement of the physics performance/relevance of this trigger, one should also take into consideration the current, \"traditional\" L1 menu seeds and compare to its efficiencies. This is done below in Section 3, but you can skip that for now if time does not permit, yet this is a crucial check to do if you want to propose a (any) new L1 trigger seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef27fc",
   "metadata": {},
   "source": [
    "## 2. bis: Hardware input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1cde3",
   "metadata": {},
   "source": [
    "The uGT/FPGA firmware does not actually use the float variables as we did above. Instead, more hardware efficient integer values are used, called \"hardware\" (HW) variables. This is important when moving towards implementing the ML model in the actual FW -> ideally the training has to be done already with the HW features. \n",
    "\n",
    "Below we will load the HW representations of the features used above and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8035e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_sig_hw = f_sig[\"Events\"].arrays(\n",
    "    filter_name = \"/(L1Mu|L1EG|L1Jet|L1EtSum)_(hwPt|hwEta|hwPhi|etSumType)/\", \n",
    "#     entry_stop = 1e1, \n",
    "    how = \"zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_bkg_hw = f_bkg[\"Events\"].arrays(\n",
    "    filter_name = \"/(L1Mu|L1EG|L1Jet|L1EtSum)_(hwPt|hwEta|hwPhi|etSumType)/\", \n",
    "#     entry_stop = 1e1, \n",
    "    how = \"zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc150a9b",
   "metadata": {},
   "source": [
    "Note that we will define a dedicated HW object dictionary for the conversion to the tabular format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_objects = [\n",
    "    {\"name\" : \"MET\", \"key\" : \"L1EtSum\", \"fields\" : [\"hwPt\", \"hwPhi\"], \"n_obj\" : 1,\n",
    "     \"cuts\" : [{\"cuttype\" : \"equals\", \"field\" : \"etSumType\", \"value\" : 2}] },  # etSumType 2 = MET noHF\n",
    "    {\"name\" : \"jet\", \"key\" : \"L1Jet\", \"fields\" : [\"hwPt\", \"hwEta\", \"hwPhi\"], \"n_obj\" : 8},\n",
    "    {\"name\" : \"eg\", \"key\" : \"L1EG\", \"fields\" : [\"hwPt\", \"hwEta\", \"hwPhi\"], \"n_obj\" : 8},\n",
    "    {\"name\" : \"muon\", \"key\" : \"L1Mu\", \"fields\" : [\"hwPt\", \"hwEta\", \"hwPhi\"], \"n_obj\" : 2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee829d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig_hw, hw_var_names = formatDataTopotrigger(data_sig_hw, hw_objects, verbosity = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a32f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bkg_hw, hw_var_names = formatDataTopotrigger(data_bkg_hw, hw_objects, verbosity = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fa871",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bkg_hw.shape, x_bkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e476d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining signal & bkg\n",
    "x_hw = np.concatenate((x_bkg_hw, x_sig_hw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3594ce0",
   "metadata": {},
   "source": [
    "Let us compare the float and int/HW values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7cca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate(var_names[:10]):\n",
    "    if \"_0_\" not in name: continue\n",
    "        \n",
    "    plt.figure()\n",
    "    \n",
    "    _ = plt.hist(x[:,i], bins = 50, log = True, density = True, label = \"float\")\n",
    "    _ = plt.hist(x_hw[:,i], bins = 50, histtype = \"step\", density = True, label = \"int/hw\")\n",
    "    \n",
    "    plt.xlabel(name)\n",
    "    plt.legend()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a365c22",
   "metadata": {},
   "source": [
    "Now we will will prepare a separate scaler for the HW values, and then we will save it for \"Part 3\" of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwScaler = StandardScaler()\n",
    "_ = hwScaler.fit(x_hw)\n",
    "x_hw_scaled = hwScaler.transform(x_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f53111",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir+\"hwScaler.pkl\", 'wb') as file_pi: \n",
    "    pickle.dump(hwScaler, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1eb31",
   "metadata": {},
   "source": [
    "And now we can compare the scaled values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,name in enumerate(var_names[:100]):\n",
    "    if \"_0_\" not in name: continue\n",
    "        \n",
    "    plt.figure()\n",
    "    \n",
    "    _ = plt.hist(x_scaled[:,i], bins = 50, log = True, density = True, label = \"float\")\n",
    "    _ = plt.hist(x_hw_scaled[:,i], bins = 50, histtype = \"step\", density = True, label = \"int/hw\")\n",
    "    \n",
    "    plt.xlabel(name)\n",
    "    plt.legend()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1869b6",
   "metadata": {},
   "source": [
    "Note that except for phi the scaled float and int/HW value distributions match rather nice.\n",
    "The mismatches for pt/eta are mostly due to rounding effects, whereas for phi the \"issue\" is that the HW phi 0 value corresponds to -pi and hence the zero-padding results in an asymmetric distribution, unlike for the float variables.\n",
    "\n",
    "Since the phi variables do not contribute much to the NN performance, we can for now ignore this inconsistency. In an actual application it is advisable to train the ML with HW variables directly though!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf00da7",
   "metadata": {},
   "source": [
    "# 3. Study current L1 menu\n",
    "\n",
    "The idea is to look at the acceptance (trigger efficiency) of the current L1 menu for our signal of interest\n",
    "\n",
    "### Get list of unprescaled L1 seeds\n",
    "\n",
    "We will load the prescale table from a recent menu e.g. https://github.com/cms-l1-dpg/L1MenuRun3/blob/master/official/L1Menu_Collisions2023_v1_2_0/PrescaleTable/L1Menu_Collisions2023_v1_2_0.csv\n",
    "(click on `RAW` view for the raw url to be used below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc395893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the Prescale (PS) table that was used for the MC generation:\n",
    "# l1_ps_url = \"https://raw.githubusercontent.com/cms-l1-dpg/L1MenuRun3/master/development/L1Menu_Collisions2022_v1_3_0/PrescaleTable/L1Menu_Collisions2022_v1_3_0.csv\"\n",
    "\n",
    "## official PS table for 2023 \n",
    "# l1_ps_url = \"https://raw.githubusercontent.com/cms-l1-dpg/L1MenuRun3/master/official/L1Menu_Collisions2023_v1_2_0/PrescaleTable/L1Menu_Collisions2023_v1_2_0.csv\"\n",
    "\n",
    "## special PS table with backup seeds disabled for 1.9E34 column -> will use that one\n",
    "l1_ps_url = \"https://raw.githubusercontent.com/cms-l1-dpg/L1MenuRun3/master/development/L1Menu_Collisions2023_v1_2_0/PrescaleTable/L1Menu_Collisions2023_v1_2_0-DedicatedPureRateStudy.csv\"\n",
    "df_l1_ps = pd.read_csv(l1_ps_url)\n",
    "df_l1_ps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de5c33",
   "metadata": {},
   "source": [
    "Select unprescaled seeds (=`1`) for column \"1p9E34\" (this is a special PS table where backup seeds are disabled in this column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4256188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter PS column\n",
    "mask = df_l1_ps[\"2p0E34\"] == 1\n",
    "# mask = df_l1_ps[\"1p9E34\"] == 1 # for special no-backup column\n",
    "l1_unps_seeds = df_l1_ps[mask].Name.values\n",
    "print(f\"We selected {len(l1_unps_seeds)} out of {len(df_l1_ps)} L1 seeds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a670e0f",
   "metadata": {},
   "source": [
    "### Load L1 seeds from the signal MC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2705811",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bits_sig = f_sig[\"Events\"].arrays(filter_name = l1_unps_seeds, entry_stop = 1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bits_sig.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a57c8",
   "metadata": {},
   "source": [
    "One can see that while we expected 161 seeds, only 178 were loaded -> the missing were not in the menu used for the MC production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a pandas dataframe for convenience\n",
    "df_bits_sig = ak.to_dataframe(bits_sig) # might be to_pandas if using awkward < v2\n",
    "df_bits_sig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb8edc",
   "metadata": {},
   "source": [
    "Now we will compute the total number of counts and in addition the number of pure counts i.e. where only one seed has fired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b08eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counts = {}\n",
    "\n",
    "for seed in df_bits_sig.columns:\n",
    "    counts[seed] = {\n",
    "        \"total\": df_bits_sig[seed].sum(), \n",
    "        \"pure\" :  (df_bits_sig[seed] & ~df_bits_sig.drop(seed, axis=1).any(axis=1)).sum(),\n",
    "}\n",
    "    \n",
    "df_counts = pd.DataFrame(counts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.sort_values(\"total\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ba473",
   "metadata": {},
   "source": [
    "Note that when we sort the seeds by the total counts as above we do have some high-count seeds with very low pure rate: these are in the \"shadow\" of other seeds when looking at this signal MC sample (but not necessarily on Background MC/ZeroBias!)\n",
    "\n",
    "We will exlcude the seeds with low pure rate and sort by total again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_pure = df_counts[df_counts.pure > 0]\n",
    "df_counts_pure.sort_values(\"total\", ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86231bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_counts_pure) # to check number of remaining seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd4cc4",
   "metadata": {},
   "source": [
    "Now we can compute the signal efficiencies by dividing the counts by the number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc616c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eff_pure = df_counts_pure/len(bits_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941417ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eff_pure.sort_values(\"total\", ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment below to make a plot of the efficiencies\n",
    "# ax = df_eff_pure.sort_values(\"total\", ascending = True).plot(kind = \"barh\", figsize = (10,12))\n",
    "# # ax.set_xscale(\"log\")\n",
    "# ax.set_xlabel(\"Signal efficiency\")\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd4b5c",
   "metadata": {},
   "source": [
    "### Rates for the unprescaled seeds using the background sample\n",
    "\n",
    "We will use the above background dataset and only load the pure seeds we selected above for speed/performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bits_bkg = f_bkg[\"Events\"].arrays(filter_name = df_counts_pure.index.values, entry_stop = 1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb47987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_bits_bkg = ak.to_dataframe(bits_bkg) # might be to_pandas if using awkward < v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counts_bkg = {}\n",
    "\n",
    "df = df_bits_bkg\n",
    "\n",
    "for seed in df.columns:\n",
    "#     print(seed)\n",
    "    counts_bkg[seed] = {\n",
    "        \"total\": df[seed].sum(), \n",
    "        \"pure\" :  (df[seed] & ~df.drop(seed, axis=1).any(axis=1)).sum(),\n",
    "\n",
    "}\n",
    "    \n",
    "df_counts_bkg = pd.DataFrame(counts_bkg).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d397ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_bkg.sort_values(\"total\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238e219",
   "metadata": {},
   "source": [
    "We convert the counts to the rate using the total minbias rate we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156879b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_bkg = df_counts_bkg * totalMinBiasRate() / len(bits_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c422757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_bkg.sort_values(\"total\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200297ed",
   "metadata": {},
   "source": [
    "You might notice that the rates are much lower than usually quoted. This is not only due to data/MC differences, but also due to the fact that here in MC we consider a wide PU range.\n",
    "\n",
    "Only for reference: rates from OMS: https://cmsoms.cern.ch/cms/triggers/l1_rates?cms_run=370293&update=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae9cf1",
   "metadata": {},
   "source": [
    "We can merge the signal efficiency and (background) rate dataframes for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eff_rate = df_eff_pure.join(df_rate_bkg, lsuffix = \" eff\", rsuffix = \" rate\")\n",
    "df_eff_rate.sort_values(\"total eff\", ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19cb83",
   "metadata": {},
   "source": [
    "Now we can make a \"ROC\" curve from the signal efficiency and rates for the L1 seeds.\n",
    "For this we will first selecte the 5 \"best\" triggers for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trigs = list(df_eff_pure.sort_values(\"total\", ascending = False)[:5].index)\n",
    "best_trigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "for trig in best_trigs:\n",
    "    eff = df_eff_rate.loc[trig,\"total eff\"]\n",
    "    rate = df_eff_rate.loc[trig,\"total rate\"]\n",
    "    print(trig, eff, rate)\n",
    "    \n",
    "    plt.plot(rate, eff, \"o\", label = trig, ms = 10)\n",
    "    \n",
    "plt.legend(fontsize = 15)\n",
    "plt.ylim(0,1)\n",
    "# plt.xlim(0,)\n",
    "plt.ylabel(\"Trigger/Signal efficiency [%]\")\n",
    "plt.xlabel(\"Trigger rate [kHz]\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71daa19f",
   "metadata": {},
   "source": [
    "# 4. Compare NN vs L1 seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4902c",
   "metadata": {},
   "source": [
    "Finally, we can compare the NN and L1 menu triggers in one plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.title(\"HH>bbtautau\")\n",
    "\n",
    "for trig in best_trigs:\n",
    "    eff = df_eff_rate.loc[trig,\"total eff\"]\n",
    "    rate = df_eff_rate.loc[trig,\"total rate\"]\n",
    "    plt.plot(rate, eff, \"o\", label = trig, ms = 10)\n",
    "    \n",
    "    \n",
    "plt.plot(fpr * totalMinBiasRate(), tpr , label = \"NN trigger\")\n",
    "    \n",
    "plt.xlim(0,20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.ylabel(\"Trigger/Signal efficiency [%]\")\n",
    "plt.xlabel(\"Trigger rate [kHz]\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3ed25",
   "metadata": {},
   "source": [
    "We can also add the OR of the full L1 menu and of the best triggers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = np.concatenate((bits_bkg, bits_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trig_or = False\n",
    "\n",
    "for trig in bits.fields:\n",
    "    all_trig_or = all_trig_or | bits[trig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bits_or = False\n",
    "\n",
    "for trig in best_trigs:\n",
    "    best_bits_or = best_bits_or | bits[trig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a \"ROC\" curve from the signal efficiency and rates\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.title(\"HH>bbtautau\")\n",
    "\n",
    "for trig in best_trigs:\n",
    "    \n",
    "    eff = df_eff_pure.loc[trig,\"total\"]\n",
    "    rate = df_rate_bkg.loc[trig,\"total\"]\n",
    "#     print(trig, eff, rate)\n",
    "    \n",
    "    plt.plot(rate, eff, \"o\", label = trig, ms = 10)\n",
    "    \n",
    "## total L1 menu\n",
    "eff = np.sum(all_trig_or[y==1]) / np.sum(y==1)\n",
    "rate = np.sum(all_trig_or[y==0]) / np.sum(y==0) * totalMinBiasRate()\n",
    "plt.plot(rate, eff, \"o\", label = \"All L1\", ms = 10)\n",
    "    \n",
    "plt.plot(fpr * totalMinBiasRate(), tpr , label = \"NN trigger\")\n",
    "    \n",
    "plt.xlim(0,50)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.ylabel(\"Trigger/Signal efficiency [%]\")\n",
    "plt.xlabel(\"Trigger rate [kHz]\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bf25",
   "metadata": {},
   "source": [
    "Note that the `All L1` menu rate is unrealistic since the MC PU profile is lower than what the considered menu was designed for!\n",
    "\n",
    "**Bonus**: we can also make a ROC curve for the L1 HT variable to check if we could lower the HT threshold and compare to the NN ROC.\n",
    "\n",
    "For this we first need to extract the L1 HT from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 l1ht\n",
    "l1ht_sig = data_sig.L1EtSum[data_sig.L1EtSum[\"etSumType\"] == 1].pt[:,0]\n",
    "l1ht_bkg = data_bkg.L1EtSum[data_bkg.L1EtSum[\"etSumType\"] == 1].pt[:,0]\n",
    "l1ht_all = np.concatenate([l1ht_bkg,l1ht_sig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(l1ht_bkg, bins = np.linspace(0,500,50), log = True, density = 1,)\n",
    "_ = plt.hist(l1ht_sig, bins = _[1], histtype = \"step\", density = 1,)\n",
    "plt.xlabel(\"L1 HT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818e868",
   "metadata": {},
   "source": [
    "Make the HT \"ROC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_fpr, ht_tpr, ht_thr = roc_curve(y, l1ht_all, drop_intermediate=False)\n",
    "# plt.plot(ht_fpr, ht_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.title(\"HH>bbtautau\")\n",
    "\n",
    "for trig in best_trigs:\n",
    "    eff = df_eff_pure.loc[trig,\"total\"]\n",
    "    rate = df_rate_bkg.loc[trig,\"total\"]\n",
    "\n",
    "    plt.plot(rate, eff, \"o\", label = trig, ms = 10)\n",
    "    \n",
    "## total L1 menu\n",
    "eff = np.sum(all_trig_or[y==1]) / np.sum(y==1)\n",
    "rate = np.sum(all_trig_or[y==0]) / np.sum(y==0) * totalMinBiasRate()\n",
    "plt.plot(rate, eff, \"o\", label = \"All L1\", ms = 10)\n",
    "\n",
    "## HT ROC\n",
    "plt.plot(ht_fpr * totalMinBiasRate(), ht_tpr, label = \"HT thr scan\")\n",
    "    \n",
    "## NN\n",
    "plt.plot(fpr * totalMinBiasRate(), tpr , label = \"NN trigger\")\n",
    "  \n",
    "plt.xlim(0,50)\n",
    "# plt.xlim(1,1e3)\n",
    "# plt.xscale(\"log\")\n",
    "\n",
    "plt.legend(fontsize = 15)\n",
    "plt.ylabel(\"Trigger/Signal efficiency [%]\")\n",
    "plt.xlabel(\"Trigger rate [kHz]\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6388b",
   "metadata": {},
   "source": [
    "You can see that the `L1_HTT280er` seed point lies exactly on the HT scan curve which is a good cross-check.\n",
    "\n",
    "Finally, the NN always outperforms the L1 Menu, even when all the seeds are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80b029",
   "metadata": {},
   "source": [
    "# 5. Offline Efficiencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71d440",
   "metadata": {},
   "source": [
    "In this part we will look into the trigger efficiencies with respect to some offline quantities such as MET and HT. \n",
    "\n",
    "This will help to see in which phase-space the NN trigger outperforms the traditional seeds.\n",
    "\n",
    "We first need to load the offline Jets and MET for the signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12dc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_sig = uproot.open(fname_sig)\n",
    "events_sig = f_sig[\"Events\"].arrays(\n",
    "    filter_name = [\"/Jet_(pt|eta|phi|jetId)/\", \"MET_pt\"],\n",
    "#     entry_stop = 1,\n",
    "    how = \"zip\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba0eb4",
   "metadata": {},
   "source": [
    "We will now compute the HT from jets with pt > 30, eta < 2.4 and jetId>=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "jets = events_sig.Jet\n",
    "jets[\"mass\"] = 0\n",
    "\n",
    "jets_mask = (jets.pt > 30) & (abs(jets.eta) < 2.4) & (events_sig.Jet.jetId >= 2)\n",
    "good_jets = ak.Array(jets[jets_mask], with_name = \"Momentum4D\")\n",
    "\n",
    "ht_sig = ak.fill_none(ak.sum(good_jets.pt, axis = 1), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91e732",
   "metadata": {},
   "source": [
    "We will plot the distribution of offline HT and then use this as the denominator.\n",
    "The numerator will be the distribution after a certain trigger selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76116590",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax1 = plt.subplots(figsize = (10,8))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel(\"Offline HT\")\n",
    "ax1.set_ylabel(\"Trigger (Signal) efficiency\")\n",
    "\n",
    "xvar = ht_sig\n",
    "xbins = np.linspace(0,1000,50)\n",
    "den = np.histogram(xvar, bins = xbins)\n",
    "ax2.hist(xvar, bins = xbins, label = \"HH>bbtautau\", color = \"grey\", alpha = 0.3)\n",
    "ax2.legend(loc = 4)\n",
    "ax2.set_ylabel(\"Event counts\")\n",
    "\n",
    "#### L1 \n",
    "for trig in best_trigs[:2]:\n",
    "    num = np.histogram(xvar[bits_sig[trig]], bins = den[1])\n",
    "    rate = np.sum(bits_bkg[trig]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = f\"{trig}, \\nRate: {rate:.1f} kHz\")\n",
    "    \n",
    "if True:\n",
    "    num = np.histogram(xvar[all_trig_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(all_trig_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"All L1 Menu, Rate: {rate:.1f} kHz\")\n",
    "    \n",
    "## BEST 5\n",
    "if True:\n",
    "    num = np.histogram(xvar[best_bits_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(best_bits_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"Top 5 L1, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "\n",
    "# ax1.set_ylim(0,1.2)\n",
    "ax1.grid()\n",
    "\n",
    "ax1.legend(loc = 5, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc5eb5",
   "metadata": {},
   "source": [
    "Now we need to make the NN prediction for the full sample, since we do not have the test/train split for the offline jets. We can do this since our model does not overtrain -> the train/test performance is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this takes about 50 secs - time for a short break\n",
    "y_pred = model.predict(x_scaled)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9156272",
   "metadata": {},
   "source": [
    "Now we add the NN for some reasonable threshold/rate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57711e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax1 = plt.subplots(figsize = (10,8))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel(\"Offline HT\")\n",
    "ax1.set_ylabel(\"Trigger (Signal) efficiency\")\n",
    "\n",
    "xvar = ht_sig\n",
    "xbins = np.linspace(0,1000,50)\n",
    "den = np.histogram(xvar, bins = xbins)\n",
    "ax2.hist(xvar, bins = xbins, label = \"HH>bbtautau\", color = \"grey\", alpha = 0.3)\n",
    "ax2.legend(loc = 4)\n",
    "ax2.set_ylabel(\"Event counts\")\n",
    "\n",
    "#### L1 \n",
    "for trig in best_trigs[:1]:\n",
    "    num = np.histogram(xvar[bits_sig[trig]], bins = den[1])\n",
    "    rate = np.sum(bits_bkg[trig]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = f\"{trig}, \\nRate: {rate:.1f} kHz\")\n",
    "    \n",
    "## All L1\n",
    "if True:\n",
    "    num = np.histogram(xvar[all_trig_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(all_trig_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"All L1 Menu, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "## BEST 5\n",
    "if True:\n",
    "    num = np.histogram(xvar[best_bits_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(best_bits_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"Top 5 L1, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "## NN trigger\n",
    "for thr in [0.985]:\n",
    "    trig_mask = (y_pred > thr)\n",
    "    num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "    rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    \n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"NN>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "ax1.grid()\n",
    "ax1.legend(loc = 5, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11b11f",
   "metadata": {},
   "source": [
    "We can look at the efficiencies also for the L1 HT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax1 = plt.subplots(figsize = (10,8))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel(\"L1 HT\")\n",
    "ax1.set_ylabel(\"Trigger (Signal) efficiency\")\n",
    "\n",
    "xvar = l1ht_sig\n",
    "xbins = np.linspace(0,600,50)\n",
    "den = np.histogram(xvar, bins = xbins)\n",
    "ax2.hist(xvar, bins = xbins, label = \"HH>bbtautau\", color = \"grey\", alpha = 0.3)\n",
    "ax2.legend(loc = 4)\n",
    "ax2.set_ylabel(\"Event counts\")\n",
    "\n",
    "#### L1 \n",
    "for trig in list(best_trigs[:1]):# + [\"L1_SingleJet180\"]:\n",
    "    num = np.histogram(xvar[bits_sig[trig]], bins = den[1])\n",
    "    rate = np.sum(bits_bkg[trig]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = f\"{trig}, Rate: {rate:.1f} kHz\")\n",
    "    \n",
    "## BEST 5\n",
    "if True:\n",
    "    num = np.histogram(xvar[best_bits_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(best_bits_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"Top 5 L1, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "## NN trigger\n",
    "for thr in [0.985]:\n",
    "    trig_mask = (y_pred > thr)\n",
    "    num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "    rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    \n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"NN>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "# ax1.set_ylim(0,1.2)\n",
    "ax1.grid()\n",
    "\n",
    "ax1.legend(loc = 5, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbc20f",
   "metadata": {},
   "source": [
    "The L1 HT280 seed naturally is a step function (~rounding effects). \n",
    "But, interestingly, the NN plateaus after the 280 GeV threshold yet it shows a better efficiency for offline HT at this score threshold!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80492a30",
   "metadata": {},
   "source": [
    "Finally we can check the efficiencies for some of the input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd085e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate(var_names):\n",
    "    if \"0_pt\" not in name: continue\n",
    "    \n",
    "    f,ax1 = plt.subplots(figsize = (10,8))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.set_xlabel(name)\n",
    "    ax1.set_ylabel(\"Trigger (Signal) efficiency\")\n",
    "\n",
    "    xvar = x_sig[:,i]\n",
    "    xbins = 50\n",
    "    den = np.histogram(xvar[xvar<300], bins = xbins)\n",
    "    ax2.hist(xvar, bins = den[1], label = \"HH>bbtautau\", color = \"grey\", alpha = 0.3)\n",
    "    ax2.legend(loc = 4)\n",
    "    ax2.set_ylabel(\"Event counts\")\n",
    "\n",
    "    #### L1 \n",
    "    for trig in best_trigs[:1]:\n",
    "        num = np.histogram(xvar[bits_sig[trig]], bins = den[1])\n",
    "        rate = np.sum(bits_bkg[trig]) / len(y_bkg) * totalMinBiasRate()\n",
    "        ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = f\"{trig}, \\nRate: {rate:.1f} kHz\")\n",
    "\n",
    "    ## ALL L1\n",
    "    if True:\n",
    "        num = np.histogram(xvar[all_trig_or[y==1]], bins = den[1])\n",
    "        rate = np.sum(all_trig_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "        ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"All L1 Menu, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "    ## BEST 5\n",
    "    if True:\n",
    "        num = np.histogram(xvar[best_bits_or[y==1]], bins = den[1])\n",
    "        rate = np.sum(best_bits_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "        ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"Top 5 L1, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "    ## NN trigger\n",
    "    for thr in [0.985]:\n",
    "        trig_mask = (y_pred > thr) #& (x[:,2] > 50)\n",
    "        num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "        rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "        pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "\n",
    "        ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"NN>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "\n",
    "    ax1.set_ylim(0,1.1)\n",
    "    ax1.grid()\n",
    "\n",
    "    ax1.legend(loc = 5, fontsize = 15)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c4cc0",
   "metadata": {},
   "source": [
    "# Bonus: BDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68239865",
   "metadata": {},
   "source": [
    "We will try out a BDT instead of an NN for the same classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt = XGBClassifier(n_estimators=20, max_depth=3, learning_rate=1, \n",
    "                        objective='binary:logistic',\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49164b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = bdt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1d3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_bdt = bdt.predict_proba(x_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21aea9",
   "metadata": {},
   "source": [
    "We will also check the ROCs for the test/train separately to check for overtraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test_bdt = bdt.predict_proba(X_test)[:,1]\n",
    "y_train_bdt = bdt.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_bdt, tpr_bdt, thr_bdt = roc_curve(y_test, y_test_bdt, drop_intermediate=False)\n",
    "plt.plot(fpr_bdt * totalMinBiasRate(), tpr_bdt , label = \"bdt test\")\n",
    "\n",
    "fpr_bdt, tpr_bdt, thr_bdt = roc_curve(y_train, y_train_bdt, drop_intermediate=False)\n",
    "plt.plot(fpr_bdt * totalMinBiasRate(), tpr_bdt , label = \"bdt train\")\n",
    "\n",
    "fpr_bdt, tpr_bdt, thr_bdt = roc_curve(y, y_bdt, drop_intermediate=False)\n",
    "plt.plot(fpr_bdt * totalMinBiasRate(), tpr_bdt , label = \"bdt all\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0,20)\n",
    "# plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18df8f2",
   "metadata": {},
   "source": [
    "Extra: feature importance from BDT/bdt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "plt.bar(var_names, bdt.feature_importances_)\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650f844",
   "metadata": {},
   "source": [
    "We can add the BDT to the ROC curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd63056",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.title(\"HH>bbtautau\")\n",
    "\n",
    "for trig in best_trigs:\n",
    "    eff = df_eff_pure.loc[trig,\"total\"]\n",
    "    rate = df_rate_bkg.loc[trig,\"total\"]\n",
    "\n",
    "    plt.plot(rate, eff, \"o\", label = trig, ms = 10)\n",
    "    \n",
    "## total L1 menu\n",
    "eff = np.sum(all_trig_or[y==1]) / np.sum(y==1)\n",
    "rate = np.sum(all_trig_or[y==0]) / np.sum(y==0) * totalMinBiasRate()\n",
    "plt.plot(rate, eff, \"o\", label = \"All L1\", ms = 10)\n",
    "\n",
    "## HT ROC\n",
    "plt.plot(ht_fpr * totalMinBiasRate(), ht_tpr, label = \"HT thr scan\")\n",
    "    \n",
    "## NN\n",
    "plt.plot(fpr * totalMinBiasRate(), tpr , label = \"NN trigger\")\n",
    "\n",
    "## BDT\n",
    "fpr_bdt, tpr_bdt, thr_bdt = roc_curve(y, y_bdt, drop_intermediate=False)\n",
    "plt.plot(fpr_bdt * totalMinBiasRate(), tpr_bdt , label = \"BDT trigger\")\n",
    "  \n",
    "plt.xlim(0,50)\n",
    "# plt.xlim(1,1e3)\n",
    "# plt.xscale(\"log\")\n",
    "\n",
    "plt.legend(fontsize = 15)\n",
    "plt.ylabel(\"Trigger/Signal efficiency [%]\")\n",
    "plt.xlabel(\"Trigger rate [kHz]\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7e094",
   "metadata": {},
   "source": [
    "We can also check the efficiencies again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30707719",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax1 = plt.subplots(figsize = (10,8))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel(\"Offline HT\")\n",
    "ax1.set_ylabel(\"Trigger (Signal) efficiency\")\n",
    "\n",
    "xvar = ht_sig\n",
    "xbins = np.linspace(0,1000,50)\n",
    "den = np.histogram(xvar, bins = xbins)\n",
    "ax2.hist(xvar, bins = xbins, label = \"HH>bbtautau\", color = \"grey\", alpha = 0.3)\n",
    "ax2.legend(loc = 4)\n",
    "ax2.set_ylabel(\"Event counts\")\n",
    "\n",
    "#### L1 \n",
    "for trig in list(best_trigs[:1]):# + [\"L1_SingleJet180\"]:\n",
    "    num = np.histogram(xvar[bits_sig[trig]], bins = den[1])\n",
    "    rate = np.sum(bits_bkg[trig]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = f\"{trig}, Rate: {rate:.1f} kHz\")\n",
    "    \n",
    "## BEST 5\n",
    "if True:\n",
    "    num = np.histogram(xvar[best_bits_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(best_bits_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"Top 5 L1, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "## NN trigger\n",
    "for thr in [0.985]:\n",
    "    trig_mask = (y_pred > thr) \n",
    "    num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "    rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    \n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"NN>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "# BDT trigger\n",
    "for thr in [0.988]:\n",
    "    trig_mask = (y_bdt > thr) \n",
    "    num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "    rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    \n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"BDT>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "\n",
    "\n",
    "# ax1.set_ylim(0,1.2)\n",
    "ax1.grid()\n",
    "\n",
    "ax1.legend(loc = 5, fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46196c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax1 = plt.subplots(figsize = (10,8))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel(\"L1 HT\")\n",
    "ax1.set_ylabel(\"Trigger (Signal) efficiency\")\n",
    "\n",
    "xvar = l1ht_sig\n",
    "xbins = np.linspace(0,1000,50)\n",
    "den = np.histogram(xvar, bins = xbins)\n",
    "ax2.hist(xvar, bins = xbins, label = \"HH>bbtautau\", color = \"grey\", alpha = 0.3)\n",
    "ax2.legend(loc = 4)\n",
    "ax2.set_ylabel(\"Event counts\")\n",
    "\n",
    "#### L1 \n",
    "for trig in list(best_trigs[:1]):# + [\"L1_SingleJet180\"]:\n",
    "    num = np.histogram(xvar[bits_sig[trig]], bins = den[1])\n",
    "    rate = np.sum(bits_bkg[trig]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = f\"{trig}, Rate: {rate:.1f} kHz\")\n",
    "    \n",
    "## BEST 5\n",
    "if True:\n",
    "    num = np.histogram(xvar[best_bits_or[y==1]], bins = den[1])\n",
    "    rate = np.sum(best_bits_or[y==0]) / len(y_bkg) * totalMinBiasRate()\n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \".-\", label = f\"Top 5 L1, Rate: {rate:.1f} kHz\")\n",
    "\n",
    "## NN trigger\n",
    "for thr in [0.985]:\n",
    "    trig_mask = (y_pred > thr) #& (l1ht_all > 150)\n",
    "    num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "    rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    \n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"NN>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "# BDT trigger\n",
    "for thr in [0.988]:\n",
    "    trig_mask = (y_bdt > thr) #& (l1ht_all > 150)\n",
    "    num = np.histogram(xvar[trig_mask[y == 1]], bins = den[1])\n",
    "\n",
    "    rate = np.sum(trig_mask[y == 0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    pure_rate = np.sum(trig_mask[y == 0] & ~all_trig_or[y==0])/ len(y_bkg) * totalMinBiasRate()\n",
    "    \n",
    "    ax1.plot(den[1][1:], num[0]/den[0], \"o-\", label = \"BDT>%s, Total Rate: %.1f kHz, Pure: %.1f\" %(thr, rate, pure_rate))\n",
    "\n",
    "\n",
    "# ax1.set_ylim(0,1.2)\n",
    "ax1.grid()\n",
    "\n",
    "ax1.legend(loc = 5, fontsize = 15)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
